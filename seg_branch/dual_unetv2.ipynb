{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from unet_v2.UNet_v2 import UNetV2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image / label root\n",
    "image_root = \"/local_data/dataset/polyp/detection/patients_complete/images/val/\"\n",
    "label_root = \"/local_data/dataset/polyp/detection/patients_complete/labels/val/\"\n",
    "\n",
    "# ---------------------- config ----------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# seg normalize\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "# PolypPVT 輸入尺寸（請改成你訓練時用的大小）\n",
    "SEG_SIZE = (352, 352)  # (W, H)\n",
    "\n",
    "NUM_CLASSES_DET = 2     # hyperplastic, adenoma\n",
    "BG_INDEX_SEG = 2        # segmentation 的背景 channel index\n",
    "IOU_THRESH_EVAL = 0.5   # mAP50\n",
    "CONF_THRESH_DET = 0.001 # 要求的 detection conf 門檻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation model\n",
    "model_seg = UNetV2(n_class=3)\n",
    "model_seg.load_state_dict(torch.load(\n",
    "    \"/nfs/P111yhchen/code/detection/seg_branch/runs/UNetv2/best.pth\",\n",
    "    map_location=\"cpu\"\n",
    "))\n",
    "model_seg.to(device).eval()\n",
    "\n",
    "# ---------------------- main loop ----------------------\n",
    "img_exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\")\n",
    "img_paths = []\n",
    "for e in img_exts:\n",
    "    img_paths.extend(glob.glob(os.path.join(image_root, e)))\n",
    "img_paths = sorted(img_paths)\n",
    "\n",
    "print(f\"#images in val: {len(img_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- helpers ----------------------\n",
    "def load_yolo_gt(label_path, img_w, img_h, num_classes=NUM_CLASSES_DET):\n",
    "    \"\"\"\n",
    "    讀取 YOLO txt labels -> list of {cls, box=[x1,y1,x2,y2]}\n",
    "    \"\"\"\n",
    "    if not os.path.exists(label_path):\n",
    "        return {c: [] for c in range(num_classes)}\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = [x.strip() for x in f.readlines() if x.strip()]\n",
    "\n",
    "    gts_per_cls = {c: [] for c in range(num_classes)}\n",
    "    if not lines:\n",
    "        return gts_per_cls\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) != 5:\n",
    "            continue\n",
    "        cls = int(float(parts[0]))\n",
    "        if cls >= num_classes:\n",
    "            continue\n",
    "        xc, yc, w, h = map(float, parts[1:])\n",
    "        xc *= img_w\n",
    "        yc *= img_h\n",
    "        w *= img_w\n",
    "        h *= img_h\n",
    "        x1 = xc - w / 2\n",
    "        y1 = yc - h / 2\n",
    "        x2 = xc + w / 2\n",
    "        y2 = yc + h / 2\n",
    "        gts_per_cls[cls].append([x1, y1, x2, y2])\n",
    "\n",
    "    return gts_per_cls\n",
    "\n",
    "def infer_seg_prob_map(img_bgr):\n",
    "    \"\"\"\n",
    "    img_bgr: (H,W,3) uint8\n",
    "    return prob_map: torch.Tensor, shape (C,H,W) on CPU\n",
    "    \"\"\"\n",
    "    start_seg_pre = time.perf_counter()\n",
    "    h0, w0 = img_bgr.shape[:2]\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, SEG_SIZE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    img_tensor = torch.from_numpy(img_resized).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    img_tensor = (img_tensor.to(device) - mean) / std\n",
    "    elapsed_seg_pre = (time.perf_counter() - start_seg_pre) * 1000\n",
    "    # print(f\"segmentation preprocess time: {elapsed_seg_pre:.3f} ms\")\n",
    "\n",
    "    start_seg = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        P1, P2 = model_seg(img_tensor)\n",
    "        P2 = F.interpolate(P2, size=SEG_SIZE,  mode='bilinear', align_corners=False)\n",
    "        # 如果你的 PolypPVT 回傳的是 list/tuple，就取最後一個\n",
    "        pred_seg = F.interpolate(P1+P2, size=(h0, w0), mode=\"bilinear\", align_corners=False)\n",
    "        prob = torch.softmax(pred_seg, dim=1)[0]  # (C,H,W)\n",
    "    elapsed_seg = (time.perf_counter() - start_seg) * 1000\n",
    "    # print(f\"segmentation inference time: {elapsed_seg:.3f} ms\")\n",
    "    \n",
    "    return prob.cpu()\n",
    "\n",
    "\n",
    "def classify_box_with_seg(prob_map, box_xyxy, bg_index=BG_INDEX_SEG):\n",
    "    \"\"\"\n",
    "    用 segmentation prob map 決定 bbox 的 class\n",
    "    prob_map: (C,H,W) torch Tensor on CPU\n",
    "    box_xyxy: [x1,y1,x2,y2] in image coords\n",
    "    return det_cls (0~NUM_CLASSES_DET-1) or None (如果框無效)\n",
    "    \"\"\"\n",
    "    C, H, W = prob_map.shape\n",
    "    x1, y1, x2, y2 = box_xyxy\n",
    "    x1 = max(int(np.floor(x1)), 0)\n",
    "    y1 = max(int(np.floor(y1)), 0)\n",
    "    x2 = min(int(np.ceil(x2)), W)\n",
    "    y2 = min(int(np.ceil(y2)), H)\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    \n",
    "    # the last channel is background\n",
    "    crop = prob_map[:-1, y1:y2, x1:x2]\n",
    "    vote = crop.sum(dim=(1,2))\n",
    "    det_cls = vote.argmax(dim=0).item()\n",
    "    #det_cls = bg_index if vote[det_cls].data==0 else det_cls\n",
    "    return det_cls\n",
    "    \n",
    "def box_iou_np(box1, box2):\n",
    "    \"\"\"\n",
    "    box1: (N,4), box2:(M,4) in xyxy\n",
    "    return IoU: (N,M)\n",
    "    \"\"\"\n",
    "    if box1.size == 0 or box2.size == 0:\n",
    "        return np.zeros((box1.shape[0], box2.shape[0]))\n",
    "\n",
    "    box1 = box1.astype(np.float32)\n",
    "    box2 = box2.astype(np.float32)\n",
    "\n",
    "    area1 = np.clip(box1[:, 2] - box1[:, 0], 0, None) * np.clip(box1[:, 3] - box1[:, 1], 0, None)\n",
    "    area2 = np.clip(box2[:, 2] - box2[:, 0], 0, None) * np.clip(box2[:, 3] - box2[:, 1], 0, None)\n",
    "\n",
    "    inter_x1 = np.maximum(box1[:, None, 0], box2[None, :, 0])\n",
    "    inter_y1 = np.maximum(box1[:, None, 1], box2[None, :, 1])\n",
    "    inter_x2 = np.minimum(box1[:, None, 2], box2[None, :, 2])\n",
    "    inter_y2 = np.minimum(box1[:, None, 3], box2[None, :, 3])\n",
    "\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, 0, None)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, 0, None)\n",
    "    inter = inter_w * inter_h\n",
    "\n",
    "    union = area1[:, None] + area2[None, :] - inter + 1e-16\n",
    "    return inter / union\n",
    "\n",
    "def compute_confusion_matrix(predictions,\n",
    "                             gt_boxes_per_image,\n",
    "                             num_classes=2,\n",
    "                             conf_th=0.25,\n",
    "                             iou_th=0.5):\n",
    "    \"\"\"\n",
    "    Return confusion matrix of shape (num_classes+1, num_classes+1)\n",
    "    rows:    predicted class (最後一列 = predicted background)\n",
    "    columns: ground-truth class (最後一欄 = GT background)\n",
    "    \"\"\"\n",
    "    bg = num_classes\n",
    "    cm = np.zeros((num_classes + 1, num_classes + 1), dtype=np.int64)\n",
    "\n",
    "    # 先把 prediction 按 image_id group 起來比較快\n",
    "    preds_by_img = {}\n",
    "    for p in predictions:\n",
    "        if p[\"score\"] < conf_th:\n",
    "            continue\n",
    "        preds_by_img.setdefault(p[\"image_id\"], []).append(p)\n",
    "\n",
    "    for img_id, gt_dict in gt_boxes_per_image.items():\n",
    "        # collect all GT boxes for this image\n",
    "        gt_boxes = []\n",
    "        gt_cls = []\n",
    "        for c in range(num_classes):\n",
    "            for b in gt_dict[c]:\n",
    "                gt_boxes.append(b)\n",
    "                gt_cls.append(c)\n",
    "        gt_boxes = np.array(gt_boxes, dtype=np.float32)\n",
    "        gt_cls = np.array(gt_cls, dtype=np.int64)\n",
    "\n",
    "        preds = preds_by_img.get(img_id, [])\n",
    "        if len(preds) == 0 and gt_boxes.size == 0:\n",
    "            continue\n",
    "\n",
    "        pred_boxes = np.array([p[\"box\"] for p in preds], dtype=np.float32) if preds else np.zeros((0, 4), dtype=np.float32)\n",
    "        pred_cls = np.array([p[\"cls\"] for p in preds], dtype=np.int64) if preds else np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "        N, M = pred_boxes.shape[0], gt_boxes.shape[0]\n",
    "\n",
    "        if N > 0 and M > 0:\n",
    "            ious = box_iou_np(pred_boxes, gt_boxes)  # (N,M)\n",
    "            matched_pred = np.zeros(N, dtype=bool)\n",
    "            matched_gt = np.zeros(M, dtype=bool)\n",
    "\n",
    "            # greedy 1-1 matching by IoU\n",
    "            while True:\n",
    "                idx = np.unravel_index(np.argmax(ious), ious.shape)\n",
    "                max_iou = ious[idx]\n",
    "                if max_iou < iou_th:\n",
    "                    break\n",
    "                pi, gj = idx\n",
    "                if matched_pred[pi] or matched_gt[gj]:\n",
    "                    ious[pi, gj] = -1.0\n",
    "                    continue\n",
    "                matched_pred[pi] = True\n",
    "                matched_gt[gj] = True\n",
    "\n",
    "                pc = int(pred_cls[pi])\n",
    "                gc = int(gt_cls[gj])\n",
    "                cm[pc, gc] += 1\n",
    "\n",
    "                ious[pi, :] = -1.0\n",
    "                ious[:, gj] = -1.0\n",
    "\n",
    "            # unmatched predictions -> predicted some class, GT background\n",
    "            for i in range(N):\n",
    "                if not matched_pred[i]:\n",
    "                    pc = int(pred_cls[i])\n",
    "                    cm[pc, bg] += 1\n",
    "\n",
    "            # unmatched GT -> predicted background, GT some class\n",
    "            for j in range(M):\n",
    "                if not matched_gt[j]:\n",
    "                    gc = int(gt_cls[j])\n",
    "                    cm[bg, gc] += 1\n",
    "\n",
    "        elif N > 0 and M == 0:\n",
    "            # all preds are FP, GT background\n",
    "            for pc in pred_cls:\n",
    "                cm[int(pc), bg] += 1\n",
    "        elif N == 0 and M > 0:\n",
    "            # all GT are FN, predicted background\n",
    "            for gc in gt_cls:\n",
    "                cm[bg, int(gc)] += 1\n",
    "\n",
    "    return cm\n",
    "\n",
    "# -------------------  Detection Metrics (AP) ------------------------------\n",
    "def prepare_gt_class_agnostic(gt_boxes_per_image):\n",
    "    gt_nocls = {}\n",
    "    for img_id, v in gt_boxes_per_image.items():\n",
    "        boxes = []\n",
    "        if isinstance(v, dict):\n",
    "            # v: {cls: [[...], ...], ...}\n",
    "            for cls, box_list in v.items():\n",
    "                if box_list is None:\n",
    "                    continue\n",
    "                for b in box_list:\n",
    "                    boxes.append(b)\n",
    "        else:\n",
    "            # 若本來就已經是 list of boxes\n",
    "            boxes = v\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            gt_nocls[img_id] = np.asarray(boxes, dtype=np.float32).reshape(-1, 4)\n",
    "        else:\n",
    "            gt_nocls[img_id] = np.zeros((0, 4), dtype=np.float32)\n",
    "    return gt_nocls\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 單一 box 對多個 boxes 的 IoU\n",
    "# box: shape (4,), boxes: shape (N, 4)\n",
    "# -------------------------------------------------\n",
    "def box_iou(box, boxes):\n",
    "    if boxes.size == 0:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    x1 = np.maximum(box[0], boxes[:, 0])\n",
    "    y1 = np.maximum(box[1], boxes[:, 1])\n",
    "    x2 = np.minimum(box[2], boxes[:, 2])\n",
    "    y2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    inter_w = np.clip(x2 - x1, a_min=0, a_max=None)\n",
    "    inter_h = np.clip(y2 - y1, a_min=0, a_max=None)\n",
    "    inter = inter_w * inter_h\n",
    "\n",
    "    area_box = (box[2] - box[0]) * (box[3] - box[1])\n",
    "    area_boxes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "    union = area_box + area_boxes - inter\n",
    "    iou = np.where(union > 0, inter / union, 0.0)\n",
    "    return iou\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 計算「不分分類」的 AP@iou_thr\n",
    "# all_predictions: list of dict\n",
    "#   {\"image_id\": str, \"cls\": int, \"score\": float, \"box\": [x1,y1,x2,y2]}\n",
    "# gt_boxes_per_image: img_id -> {cls: [[x1,y1,x2,y2], ...]}\n",
    "# -------------------------------------------------\n",
    "def compute_ap_class_agnostic(all_predictions, gt_boxes_per_image, iou_thr=0.5):\n",
    "    \"\"\"\n",
    "    class-agnostic 的 AP 計算：\n",
    "    - 忽略 pred[\"cls\"]，只看 image_id / box / score。\n",
    "    - 回傳 AP + 原始 PR curve + COCO 插值後 PR + 對應的 score threshold。\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ap : float\n",
    "        指定 IoU 門檻下的 Average Precision。\n",
    "    recalls : np.ndarray, shape (N,)\n",
    "        依照 score 從高到低加點時，每一點的 recall。\n",
    "    precisions : np.ndarray, shape (N,)\n",
    "        依照 score 從高到低加點時，每一點的 precision。\n",
    "    rec_points : np.ndarray, shape (101,)\n",
    "        COCO 風格的固定 recall 取樣點：0.00, 0.01, ..., 1.00。\n",
    "    prec_interp : np.ndarray, shape (101,)\n",
    "        對應到 rec_points 的「插值後」precision（用來算 AP 的 PR 曲線）。\n",
    "    thresholds : np.ndarray, shape (N,)\n",
    "        每一個 PR 點對應的 score threshold（第 i 個點 = 保留 score ≥ thresholds[i]）。\n",
    "    \"\"\"\n",
    "    # 先把 GT 合併成「不分 class」版本\n",
    "    gt_nocls = prepare_gt_class_agnostic(gt_boxes_per_image)\n",
    "\n",
    "    # 總 GT 數量 (所有圖、所有類別加總)\n",
    "    npos = sum(len(b) for b in gt_nocls.values())\n",
    "    if npos == 0:\n",
    "        return float(\"nan\"), None, None, None, None, None\n",
    "\n",
    "    # 依照 score 由大到小排序 (完全忽略 cls)\n",
    "    preds = sorted(all_predictions, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    tp = np.zeros(len(preds), dtype=np.float32)\n",
    "    fp = np.zeros(len(preds), dtype=np.float32)\n",
    "    thresholds = np.array([p[\"score\"] for p in preds], dtype=np.float32)\n",
    "\n",
    "    # 每張圖的每個 GT 只能被 match 一次\n",
    "    gt_used = {img_id: np.zeros(len(boxes), dtype=bool)\n",
    "               for img_id, boxes in gt_nocls.items()}\n",
    "\n",
    "    for i, p in enumerate(preds):\n",
    "        img_id = p[\"image_id\"]\n",
    "        box = np.asarray(p[\"box\"], dtype=np.float32)\n",
    "\n",
    "        gt_boxes = gt_nocls.get(img_id, None)\n",
    "        if gt_boxes is None or len(gt_boxes) == 0:\n",
    "            # 這張圖沒有 GT，任何預測都是 FP\n",
    "            fp[i] = 1.0\n",
    "            continue\n",
    "\n",
    "        ious = box_iou(box, gt_boxes)\n",
    "        max_iou_idx = int(np.argmax(ious))\n",
    "        max_iou = float(ious[max_iou_idx])\n",
    "\n",
    "        if max_iou >= iou_thr and not gt_used[img_id][max_iou_idx]:\n",
    "            tp[i] = 1.0\n",
    "            gt_used[img_id][max_iou_idx] = True\n",
    "        else:\n",
    "            fp[i] = 1.0\n",
    "\n",
    "    # ------- 原始 PR curve（每加一個預測點更新一次） -------\n",
    "    tp_cum = np.cumsum(tp)\n",
    "    fp_cum = np.cumsum(fp)\n",
    "\n",
    "    recalls = tp_cum / npos\n",
    "    precisions = tp_cum / np.maximum(tp_cum + fp_cum, 1e-8)\n",
    "\n",
    "    # ------- COCO 風格：在 0~1 的 101 個 recall 點做插值 -------\n",
    "    rec_points = np.linspace(0.0, 1.0, 101)\n",
    "    prec_interp = np.zeros_like(rec_points)\n",
    "\n",
    "    for idx, r in enumerate(rec_points):\n",
    "        # 找到所有 recall >= r 的點，取其中最大的 precision\n",
    "        mask = recalls >= r\n",
    "        if np.any(mask):\n",
    "            prec_interp[idx] = np.max(precisions[mask])\n",
    "        else:\n",
    "            prec_interp[idx] = 0.0\n",
    "\n",
    "    ap = float(np.mean(prec_interp))\n",
    "\n",
    "    return ap, recalls, precisions, rec_points, prec_interp, thresholds\n",
    "\n",
    "def find_best_f1_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    根據原始 PR curve 的每個點計算 F1，找出 F1 最大的點。\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_thresh : float\n",
    "        讓 F1 最大的 confidence threshold（score）。\n",
    "    best_f1 : float\n",
    "        最大的 F1 值。\n",
    "    best_p : float\n",
    "        該 threshold 底下的 precision。\n",
    "    best_r : float\n",
    "        該 threshold 底下的 recall。\n",
    "    \"\"\"\n",
    "    # F1 = 2PR / (P+R)\n",
    "    denom = precisions + recalls\n",
    "    f1 = np.where(denom > 0, 2 * precisions * recalls / denom, 0.0)\n",
    "\n",
    "    if len(f1) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    best_idx = int(np.argmax(f1))\n",
    "    best_thresh = float(thresholds[best_idx])\n",
    "    best_f1 = float(f1[best_idx])\n",
    "    best_p = float(precisions[best_idx])\n",
    "    best_r = float(recalls[best_idx])\n",
    "    return best_thresh, best_f1, best_p, best_r\n",
    "    \n",
    "\n",
    "# -------------------------------------------------\n",
    "# 實際計算 AP@50, AP@75, AP@50:95\n",
    "# all_predictions / gt_boxes_per_image 用你 main loop 算好的那兩個變數\n",
    "# -------------------------------------------------\n",
    "# AP@50:95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'v8':  ['n', 's', 'm'],\n",
    "    'v9':  ['t', 's', 'm'],\n",
    "    'v10': ['n', 's', 'm'],\n",
    "    'v11': ['n', 's', 'm'],\n",
    "    'v12': ['n', 's', 'm'],\n",
    "    'v13': ['n', 's'],\n",
    "}\n",
    "\n",
    "for k in model_dict.keys():\n",
    "    for s in model_dict[k]:\n",
    "        # detection model (YOLOv13-n)\n",
    "\n",
    "        model_path = f'/nfs/P111yhchen/code/detection/det_branch/{k}/yolo{k}{s}_single/weights/best.pt'\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "        print(f'yolo{k}{s}')\n",
    "        model_det = YOLO(model_path)\n",
    "\n",
    "        all_predictions = []  # 全部 pred bbox\n",
    "        gt_boxes_per_image = {}  # img_id -> {cls: [[x1,y1,x2,y2], ...]}\n",
    "\n",
    "        for img_path in tqdm(img_paths, desc=\"Evaluating dual-path\"):\n",
    "            img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            h0, w0 = img.shape[:2]\n",
    "\n",
    "            # ------ GT ------\n",
    "            label_path = os.path.join(label_root, img_id + \".txt\")\n",
    "            gt_boxes_per_image[img_id] = load_yolo_gt(label_path, w0, h0)\n",
    "\n",
    "            # ------ segmentation ------\n",
    "            prob_map = infer_seg_prob_map(img)  # (C,H,W)\n",
    "            \n",
    "\n",
    "            # ------ detection (YOLO) ------\n",
    "            # Ultralytics: conf threshold 在這裡設定\n",
    "            results = model_det.predict(\n",
    "                img,  # BGR numpy\n",
    "                single_cls=True,\n",
    "                conf=CONF_THRESH_DET,\n",
    "                iou=0.7,     # NMS 的 IoU 門檻，依需要調\n",
    "                verbose=False,\n",
    "                device=device,\n",
    "            )\n",
    "            \n",
    "            r = results[0]\n",
    "            if r.boxes is None or len(r.boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = r.boxes.xyxy.cpu().numpy()\n",
    "            scores = r.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for box, score in zip(boxes, scores):\n",
    "                start_map = time.perf_counter()\n",
    "                det_cls = classify_box_with_seg(prob_map, box)\n",
    "                elapsed_map = (time.perf_counter() - start_map) * 1000\n",
    "                # print(f\"map and vote time: {elapsed_map:.3f} ms\")\n",
    "                if det_cls is None:\n",
    "                    continue\n",
    "                all_predictions.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"cls\": int(det_cls),\n",
    "                    \"score\": float(score),\n",
    "                    \"box\": box.tolist(),\n",
    "                })\n",
    "\n",
    "        aps = []\n",
    "        thrs = [] # conf_thr with best F1 score\n",
    "        rs = [] \n",
    "        ps = []\n",
    "        for thr in np.arange(0.5, 1.0, 0.05):  # 0.50, 0.55, ..., 0.95\n",
    "            ap_i, r_i, p_i, _, _, thr_i = compute_ap_class_agnostic(\n",
    "                all_predictions, gt_boxes_per_image, iou_thr=thr\n",
    "            )\n",
    "            aps.append(ap_i)\n",
    "            thrs.append(thr_i)\n",
    "            rs.append(r_i)\n",
    "            ps.append(p_i)\n",
    "        ap_50_95 = float(np.mean(aps))\n",
    "        best_thr_50, best_f1_50, best_p_50, best_r_50 = find_best_f1_threshold(ps[0], rs[0], thrs[0])\n",
    "        cm = compute_confusion_matrix(\n",
    "            all_predictions,\n",
    "            gt_boxes_per_image,\n",
    "            num_classes=NUM_CLASSES_DET,\n",
    "            conf_th=best_thr_50,   # 想和 ultralytics 一樣就設 0.25\n",
    "            iou_th=0.5\n",
    "        )\n",
    "        print(\"\\nConfusion matrix @50 (rows=pred, cols=gt, last index = background):\")\n",
    "        print(cm)\n",
    "        for ci in range(NUM_CLASSES_DET):\n",
    "            pp = cm[ci, ci]/cm[ci].sum()\n",
    "            rr = cm[ci, ci]/cm[:, ci].sum()\n",
    "            print(f\"[class {ci}] precision: {pp:.4f}  recall: {rr:.4f} \")\n",
    "\n",
    "        print(f\"AP@50:95: {ap_50_95:.4f}\")\n",
    "\n",
    "        print(f\"[IoU=0.5] best threshold = {best_thr_50:.4f}\")\n",
    "        print(f\"[IoU=0.5] AP = {aps[0]:.4f}\")\n",
    "        print(f\"[IoU=0.5] precision = {best_p_50:.4f}, recall = {best_r_50:.4f}\")\n",
    "        print(f\"[IoU=0.5] best F1 = {best_f1_50:.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/P111yhchen/anaconda3/envs/yolov13/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nfs/P111yhchen/anaconda3/envs/yolov13/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/nfs/P111yhchen/anaconda3/envs/yolov13/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/nfs/P111yhchen/anaconda3/envs/yolov13/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/pvtv2.py:387: UserWarning: Overwriting pvt_v2_b0 in registry with unet_v2.pvtv2.pvt_v2_b0. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/pvtv2.py:397: UserWarning: Overwriting pvt_v2_b1 in registry with unet_v2.pvtv2.pvt_v2_b1. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/pvtv2.py:405: UserWarning: Overwriting pvt_v2_b2 in registry with unet_v2.pvtv2.pvt_v2_b2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/pvtv2.py:413: UserWarning: Overwriting pvt_v2_b3 in registry with unet_v2.pvtv2.pvt_v2_b3. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/pvtv2.py:421: UserWarning: Overwriting pvt_v2_b4 in registry with unet_v2.pvtv2.pvt_v2_b4. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/pvtv2.py:430: UserWarning: Overwriting pvt_v2_b5 in registry with unet_v2.pvtv2.pvt_v2_b5. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/P111yhchen/detection/seg_branch/UNetv2/unet_v2/UNet_v2.py:70: UserWarning: ================please provide the pretrained pvt model. Not using pretrained model.================\n",
      "  warnings.warn(warn_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov8n\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 4.097, detect params: 3.011\n",
      "total macs: 14.181, total params: 28.140\n",
      "segment inference: 20.963 ms\n",
      "detect inference: 5.232 ms\n",
      "yolov8s\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 14.323, detect params: 11.136\n",
      "total macs: 24.407, total params: 36.265\n",
      "segment inference: 21.068 ms\n",
      "detect inference: 5.355 ms\n",
      "yolov8m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 39.533, detect params: 25.857\n",
      "total macs: 49.617, total params: 50.986\n",
      "segment inference: 21.219 ms\n",
      "detect inference: 7.201 ms\n",
      "yolov9t\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 3.924, detect params: 2.006\n",
      "total macs: 14.008, total params: 27.135\n",
      "segment inference: 21.360 ms\n",
      "detect inference: 18.502 ms\n",
      "yolov9s\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 13.693, detect params: 7.288\n",
      "total macs: 23.776, total params: 32.417\n",
      "segment inference: 21.307 ms\n",
      "detect inference: 18.653 ms\n",
      "yolov9m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 38.774, detect params: 20.159\n",
      "total macs: 48.858, total params: 45.288\n",
      "segment inference: 21.518 ms\n",
      "detect inference: 13.793 ms\n",
      "yolov10n\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 4.196, detect params: 2.707\n",
      "total macs: 14.280, total params: 27.836\n",
      "segment inference: 21.424 ms\n",
      "detect inference: 8.954 ms\n",
      "yolov10s\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 12.385, detect params: 8.067\n",
      "total macs: 22.469, total params: 33.196\n",
      "segment inference: 21.306 ms\n",
      "detect inference: 9.259 ms\n",
      "yolov10m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 31.984, detect params: 16.485\n",
      "total macs: 42.068, total params: 41.614\n",
      "segment inference: 21.405 ms\n",
      "detect inference: 11.653 ms\n",
      "yolov11n\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 3.220, detect params: 2.590\n",
      "total macs: 13.304, total params: 27.719\n",
      "segment inference: 21.424 ms\n",
      "detect inference: 7.657 ms\n",
      "yolov11s\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 10.774, detect params: 9.428\n",
      "total macs: 20.858, total params: 34.557\n",
      "segment inference: 21.369 ms\n",
      "detect inference: 7.835 ms\n",
      "yolov11m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 33.005, detect params: 19.714\n",
      "total macs: 43.089, total params: 44.843\n",
      "segment inference: 21.391 ms\n",
      "detect inference: 7.924 ms\n",
      "yolov12n\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 2.989, detect params: 2.520\n",
      "total macs: 13.073, total params: 27.649\n",
      "segment inference: 21.524 ms\n",
      "detect inference: 13.488 ms\n",
      "yolov12s\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 9.788, detect params: 9.097\n",
      "total macs: 19.872, total params: 34.226\n",
      "segment inference: 21.512 ms\n",
      "detect inference: 13.583 ms\n",
      "yolov12m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 29.602, detect params: 19.568\n",
      "total macs: 39.685, total params: 44.697\n",
      "segment inference: 21.515 ms\n",
      "detect inference: 13.629 ms\n",
      "yolov13n\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 3.145, detect params: 2.460\n",
      "total macs: 13.229, total params: 27.589\n",
      "segment inference: 21.624 ms\n",
      "detect inference: 17.857 ms\n",
      "yolov13s\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_convtNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "segment macs: 10.084, segment params: 25.129\n",
      "detect macs: 10.178, detect params: 9.023\n",
      "total macs: 20.262, total params: 34.152\n",
      "segment inference: 21.486 ms\n",
      "detect inference: 17.733 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "from albumentations import RandomRotate90,Resize\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from unet_v2.UNet_v2 import UNetV2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# segmentation model\n",
    "model_seg = UNetV2(n_class=3)\n",
    "model_seg.load_state_dict(torch.load(\n",
    "    \"/nfs/P111yhchen/code/detection/seg_branch/runs/UNetv2/best.pth\",\n",
    "    map_location=\"cpu\"\n",
    "))\n",
    "model_seg.to(device).eval()\n",
    "\n",
    "model_dict = {\n",
    "    'v8':  ['n', 's', 'm'],\n",
    "    'v9':  ['t', 's', 'm'],\n",
    "    'v10': ['n', 's', 'm'],\n",
    "    'v11': ['n', 's', 'm'],\n",
    "    'v12': ['n', 's', 'm'],\n",
    "    'v13': ['n', 's'],\n",
    "}\n",
    "\n",
    "for k in model_dict.keys():\n",
    "    for s in model_dict[k]:\n",
    "        seg_time_records = []\n",
    "        det_time_records = []\n",
    "\n",
    "        # detection model\n",
    "        model_path = f'/nfs/P111yhchen/code/detection/det_branch/{k}/yolo{k}{s}_single/weights/best.pt'\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "        print(f'yolo{k}{s}')\n",
    "        yolo_model = YOLO(model_path)\n",
    "        model_det = yolo_model.model.to(device)\n",
    "        model_det.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_det = torch.randn(1, 3, 640, 640).to(device)\n",
    "            macs_det, params_det = profile(model_det, inputs=(input_det, ))\n",
    "            input_seg = torch.randn(1, 3, 352, 352).to(device)\n",
    "            macs_seg, params_seg = profile(model_seg, inputs=(input_seg, ))\n",
    "\n",
    "            print(f\"segment macs: {macs_seg/1e9:.3f}, segment params: {params_seg/1e6:.3f}\")\n",
    "            print(f\"detect macs: {macs_det/1e9:.3f}, detect params: {params_det/1e6:.3f}\")\n",
    "            print(f\"total macs: {(macs_seg + macs_det)/1e9:.3f}, total params: {(params_seg+params_det)/1e6:.3f}\")\n",
    "\n",
    "\n",
    "            for i in range(500):\n",
    "                input_det = torch.randn(1, 3, 640, 640).to(device)\n",
    "                input_seg = torch.randn(1, 3, 352, 352).to(device)\n",
    "\n",
    "                start_seg = time.perf_counter()\n",
    "                res = model_seg(input_seg)\n",
    "                seg_time_records.append(1000*(time.perf_counter()-start_seg))\n",
    "\n",
    "                start_det = time.perf_counter()\n",
    "                res = model_det(input_det)\n",
    "                det_time_records.append(1000*(time.perf_counter()-start_det))\n",
    "            seg_time_records.sort()\n",
    "            det_time_records.sort()\n",
    "            mean_seg_time = sum(seg_time_records[100:400])/len(seg_time_records[100:400])\n",
    "            mean_det_time = sum(det_time_records[100:400])/len(det_time_records[100:400])\n",
    "            print(f\"segment inference: {mean_seg_time:.3f} ms\")\n",
    "            print(f\"detect inference: {mean_det_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
